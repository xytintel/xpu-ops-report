../pytorch//build/aten/src/ATen/RegisterCUDA_0.cpp
../pytorch//build/aten/src/ATen/RegisterSparseCUDA_0.cpp
../pytorch//build/aten/src/ATen/RegisterSparseCsrCUDA_0.cpp
../pytorch//build/aten/src/ATen/RegisterNestedTensorCUDA_0.cpp
../pytorch//build/xpu/ATen/RegisterXPU_0.cpp
../pytorch//build/xpu/ATen/RegisterSparseXPU_0.cpp
../pytorch/third_party/torch-xpu-ops/src/ATen/native/sparse/SparseCsrTensor.cpp
../pytorch//build/xpu/ATen/RegisterNestedTensorXPU_0.cpp
============ CUDA ============
basic_keys: 1231
sparse_keys: 171
sparse_csr_keys: 139
nested_tensor_keys: 98
============ XPU ============
basic_keys: 1088
sparse_keys: 151
sparse_csr_keys: 1
nested_tensor_keys: 88
 
============ basic_keys ============
_cholesky_solve_helper
_conv_depthwise2d
_conv_depthwise2d.out
_convert_indices_from_coo_to_csr
_convert_indices_from_coo_to_csr.out
_convert_indices_from_csr_to_coo
_convert_indices_from_csr_to_coo.out
_cslt_compress
_cslt_sparse_mm
_cslt_sparse_mm_search
_cudnn_ctc_loss
_cudnn_ctc_loss.Tensor
_cudnn_init_dropout_state
_cudnn_rnn
_cudnn_rnn_backward
_cudnn_rnn_flatten_weight
_efficient_attention_backward
_efficient_attention_forward
_fft_c2c
_fft_c2c.out
_fft_c2r
_fft_c2r.out
_fft_r2c
_fft_r2c.out
_fill_mem_eff_dropout_mask_
_flash_attention_backward
_flash_attention_forward
_fused_sdp_choice
_int_mm
_int_mm.out
_jagged_to_padded_dense_forward
_linalg_det
_linalg_det.result
_linalg_eigh
_linalg_eigh.eigenvalues
_linalg_eigvals
_linalg_slogdet
_linalg_slogdet.sign
_linalg_solve_ex
_linalg_solve_ex.result
_linalg_svd
_linalg_svd.U
_mixed_dtypes_linear
_padded_dense_to_jagged_forward
_scaled_dot_product_cudnn_attention
_scaled_dot_product_cudnn_attention_backward
_scaled_dot_product_efficient_attention
_scaled_dot_product_efficient_attention_backward
_scaled_dot_product_flash_attention
_scaled_dot_product_flash_attention_backward
_scaled_mm
_scaled_mm.out
_slow_conv2d_backward.grad_input
_slow_conv2d_backward.output_mask
_slow_conv2d_forward
_slow_conv2d_forward.output
_sparse_semi_structured_addmm
_sparse_semi_structured_apply
_sparse_semi_structured_apply_dense
_sparse_semi_structured_linear
_sparse_semi_structured_mm
_sparse_semi_structured_tile
_to_sparse_bsc
_to_sparse_bsr
_to_sparse_csc
_to_sparse_csr
_to_sparse_semi_structured
_transformer_encoder_layer_fwd
_triton_multi_head_attention
_triton_scaled_dot_attention
_use_cudnn_ctc_loss
_use_cudnn_ctc_loss.Tensor
_validate_compressed_sparse_indices
cholesky
cholesky.out
cholesky_inverse
cholesky_inverse.out
conv_depthwise3d
convolution_backward
cudnn_affine_grid_generator
cudnn_affine_grid_generator_backward
cudnn_batch_norm
cudnn_batch_norm_backward
cudnn_convolution
cudnn_convolution.out
cudnn_convolution_add_relu
cudnn_convolution_relu
cudnn_convolution_transpose
cudnn_grid_sampler
cudnn_grid_sampler_backward
dot
geqrf
geqrf.a
linalg_cholesky_ex
linalg_cholesky_ex.L
linalg_eig
linalg_eig.out
linalg_eigvals.out
linalg_householder_product
linalg_householder_product.out
linalg_inv_ex
linalg_inv_ex.inverse
linalg_ldl_factor_ex
linalg_ldl_factor_ex.out
linalg_ldl_solve
linalg_ldl_solve.out
linalg_lstsq.out
linalg_lu
linalg_lu.out
linalg_lu_factor_ex
linalg_lu_factor_ex.out
linalg_lu_solve
linalg_lu_solve.out
linalg_matrix_exp
linalg_qr
linalg_qr.out
linalg_solve_triangular
linalg_solve_triangular.out
lu_unpack
lu_unpack.out
miopen_batch_norm
miopen_batch_norm_backward
miopen_convolution
miopen_convolution_add_relu
miopen_convolution_relu
miopen_convolution_transpose
miopen_depthwise_convolution
miopen_rnn
miopen_rnn_backward
nonzero_static
nonzero_static.out
ormqr
ormqr.out
slow_conv_dilated2d
slow_conv_dilated3d
slow_conv_transpose2d
slow_conv_transpose2d.out
slow_conv_transpose3d
slow_conv_transpose3d.out
sspaddmm.out
triangular_solve
triangular_solve.X
vdot
============ sparse_keys ============
_sparse_log_softmax
_sparse_log_softmax_backward_data
_sparse_softmax
_sparse_softmax_backward_data
_sparse_sparse_matmul
_sparse_sum_backward
_to_sparse_bsc
_to_sparse_bsr
_to_sparse_csc
_to_sparse_csr
addmm
addmm.out
addmm_
bmm
bmm.out
hspmm
hspmm.out
mm
mm.out
sspaddmm.out
============ sparse_csr_keys ============
_conj_physical
_nnz
_sparse_csr_prod.dim_dtype
_sparse_csr_sum.dim_dtype
_spsolve
_to_dense
_to_sparse
_to_sparse.sparse_dim
_to_sparse_bsc
_to_sparse_bsr
_to_sparse_csc
_to_sparse_csr
abs
abs.out
abs_
add.Tensor
add.out
add_.Tensor
addmm
addmm.out
addmv.out
angle
angle.out
asin
asin.out
asin_
asinh
asinh.out
asinh_
atan
atan.out
atan_
atanh
atanh.out
atanh_
baddbmm.out
bmm.out
ccol_indices
ceil
ceil.out
ceil_
clone
col_indices
conj_physical.out
conj_physical_
copy_
crow_indices
deg2rad
deg2rad.out
deg2rad_
dense_dim
empty.memory_format
empty_like
erf
erf.out
erf_
erfinv
erfinv.out
erfinv_
expm1
expm1.out
expm1_
fill_.Scalar
floor
floor.out
floor_
frac
frac.out
frac_
isinf
isnan
isneginf
isneginf.out
isposinf
isposinf.out
log1p
log1p.out
log1p_
mm
mm.out
mul.Scalar
mul.Tensor
mul.out
mul_.Scalar
mul_.Tensor
neg
neg.out
neg_
normal_
rad2deg
rad2deg.out
rad2deg_
relu
relu_
resize_
resize_as_sparse_
round
round.out
round_
row_indices
select.int
select_copy.int
sgn
sgn.out
sgn_
sign
sign.out
sign_
signbit
signbit.out
sin
sin.out
sin_
sinh
sinh.out
sinh_
sparse_dim
sparse_mask
sparse_sampled_addmm
sparse_sampled_addmm.out
sqrt
sqrt.out
sqrt_
sum
sum.dim_IntList
tan
tan.out
tan_
tanh
tanh.out
tanh_
threshold_backward
threshold_backward.grad_input
triangular_solve.X
trunc
trunc.out
trunc_
values
zero_
============ nested_tensor_keys ============
_fused_sdp_choice
_nested_tensor_softmax_with_shape
_scaled_dot_product_efficient_attention
_scaled_dot_product_flash_attention
_scaled_dot_product_flash_attention_backward
_test_autograd_multiple_dispatch.fullcoverage
_test_autograd_multiple_dispatch.ntonly
_transformer_encoder_layer_fwd
bmm
to_padded_tensor
